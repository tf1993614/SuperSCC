{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be30763f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:52:50.129981Z",
     "iopub.status.busy": "2025-05-14T12:52:50.129628Z",
     "iopub.status.idle": "2025-05-14T12:53:10.749362Z",
     "shell.execute_reply": "2025-05-14T12:53:10.748885Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import BadRequestError, RateLimitError\n",
    "\n",
    "from typing import List\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import basename, exists\n",
    "import re\n",
    "from functools import reduce\n",
    "import sys\n",
    "import time\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type\n",
    "\n",
    "import dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from SuperSCC import list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f545281b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:10.752052Z",
     "iopub.status.busy": "2025-05-14T12:53:10.751634Z",
     "iopub.status.idle": "2025-05-14T12:53:10.754412Z",
     "shell.execute_reply": "2025-05-14T12:53:10.754019Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/fengtang/jupyter_notebooks/working_script/evulate_feature_selection/2nd_submssion/\")\n",
    "\n",
    "# local = \"/home/fengtang/jupyter_notebooks/working_script/evulate_feature_selection/2nd_submssion/markers_for_run_GPT_on_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56cb27ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:10.756156Z",
     "iopub.status.busy": "2025-05-14T12:53:10.755921Z",
     "iopub.status.idle": "2025-05-14T12:53:10.765608Z",
     "shell.execute_reply": "2025-05-14T12:53:10.765200Z"
    }
   },
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "    GeneSetName: List[str] = Field(..., description = \"The name of the gene set being evaluated\")\n",
    "    GeneList: List[list] = Field(..., description = \"Comma-separated list of genes in the set\")\n",
    "    RelevantGeneRatio: List[float] = Field(..., description = \"The proportion of relevant genes in the set\")\n",
    "    BiologicalRelevanceScore: List[float] = Field(..., description = \"Derived from Gene Ontology and KEGG pathways, reflecting the biological function of the gene set\")\n",
    "    Pvalue: List[list] = Field(..., description = \"A value from the statistical comparison\")\n",
    "    SetvsSet: List[list] = Field(..., description = \"Gene set names for comparison\")\n",
    "    Summary: List[str] = Field(..., description = \"A brief summary of the gene functions or pathway associations for each gene in the gene set\")\n",
    "    Conclusion: List[str] = Field(..., description = \"A clear conclusion to indicate the gene set (e.g gene set1) as a better representative of that specific cell type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce5b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:10.771096Z",
     "iopub.status.busy": "2025-05-14T12:53:10.770901Z",
     "iopub.status.idle": "2025-05-14T12:53:10.777581Z",
     "shell.execute_reply": "2025-05-14T12:53:10.777178Z"
    }
   },
   "outputs": [],
   "source": [
    "def llm_compare_gene_set(model, api_key, base_url, temperature, cell_type, SuperSCC_gene_set, Seurat_wilcox_gene_set, Seurat_roc_gene_set, Scanpy_t_test_gene_set, Scanpy_log_reg_gene_set,\n",
    "                         structure_output = True, system = None, parser = None):\n",
    "    \n",
    "    llm = ChatOpenAI(model = model, \n",
    "                     temperature = temperature, \n",
    "                     api_key = api_key,\n",
    "                     base_url= base_url,\n",
    "                     max_retries = 20)\n",
    "    \n",
    "    if system == None:\n",
    "        system = \"\"\"\n",
    "            Suppose you are an insightful biologist tasked with evaluating multiple gene sets to determine which one better reflects the underlying biological function. \n",
    "            You will use both Gene Ontology and KEGG databases to design scoring metrics. \n",
    "            If cell type labels are provided, evaluate which gene set is a better representative of that specific cell type. \n",
    "            Gene Set Format: Input gene sets can be in gene symbol or Ensembl ID format. If Ensembl IDs are provided, automatically convert them to gene symbols, ensuring the accuracy of the conversion. \n",
    "            For comparison bewteen each pair of gene sets, use a statistical test like Fisher’s exact test (or chi-squared test if applicable), ensuring that the calculation detail is shown and accuracy is guaranteed (e.g. make sure 2x2 contingency table is used for Fisher’s exact test ). \n",
    "            Evaluation Method (Scoring Metrics):  \n",
    "                - Relevant Gene Ratio: The proportion of relevant genes in each gene set, should be numeric value range from 0 to 1. \n",
    "                - Biological Relevance Score: Derived from Gene Ontology and KEGG pathways, reflecting the biological function of the gene set, should be numeric value range from 0 to 1. \n",
    "                - Also the evaluation should be independent of gene set order. Normalize the ratio to account for any differences in gene set size. \n",
    "            Output requirements:  \n",
    "                - GeneSetName: The name of the gene set being evaluated. \n",
    "                - GeneList: Comma-separated list of genes in the set.\n",
    "                - RelevantGeneRatio: The proportion of relevant genes in the set. \n",
    "                - Pvalue: A value from the statistical comparison. When statistical test can not be done, should return 'None'.\n",
    "                - BiologicalRelevanceScore: Based on Gene Ontology and KEGG database associations. \n",
    "                - Summary: A brief summary of the gene functions or pathway associations for each gene in the gene set. \n",
    "                - SetvsSet: Gene set names for comparison (e.g. gene_set1 vs gene_set2)\n",
    "                - Conclusion: a clear conclusion to indicate which gene set name as a better representative of that specific cell type and also summarize the reason.\n",
    "\n",
    "            cell_type: {cell_type}\n",
    "            \n",
    "            SuperSCC_gene_set: {SuperSCC_gene_set}\n",
    "            Seurat_wilcox_gene_set: {Seurat_wilcox_gene_set}\n",
    "            Seurat_roc_gene_set: {Seurat_roc_gene_set}\n",
    "            Scanpy_t_test_gene_set: {Scanpy_t_test_gene_set}\n",
    "            Scanpy_log_reg_gene_set: {Scanpy_log_reg_gene_set}\n",
    "\n",
    "            <format_instruction>\n",
    "            {format_instructions}\n",
    "            </format_instruction>\n",
    "            \"\"\"\n",
    "    else:\n",
    "        system = system\n",
    "    \n",
    "    if parser == None:\n",
    "        parser = PydanticOutputParser(pydantic_object = Output)\n",
    "    else:\n",
    "        parser = PydanticOutputParser(pydantic_object = parser)\n",
    "\n",
    "    prompt = PromptTemplate(template = system, \n",
    "                            input_variables=[\"cell_type\", \"SuperSCC_gene_set\", \"Seurat_wilcox_gene_set\", \"Seurat_roc_gene_set\",\n",
    "                                             \"Scanpy_t_test_gene_set\", \"Scanpy_log_reg_gene_set\"], \n",
    "                            partial_variables = {\"format_instructions\": parser.get_format_instructions()})\n",
    "    \n",
    "    if structure_output:\n",
    "        chain = prompt | llm | parser\n",
    "    else:\n",
    "        chain = prompt | llm \n",
    "    \n",
    "    res = chain.invoke({\"cell_type\": cell_type, \"SuperSCC_gene_set\": SuperSCC_gene_set, \"Seurat_wilcox_gene_set\": Seurat_wilcox_gene_set, \"Seurat_roc_gene_set\": Seurat_roc_gene_set,\n",
    "                        \"Scanpy_t_test_gene_set\": Scanpy_t_test_gene_set, \"Scanpy_log_reg_gene_set\": Scanpy_log_reg_gene_set})\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc335cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:10.779164Z",
     "iopub.status.busy": "2025-05-14T12:53:10.779014Z",
     "iopub.status.idle": "2025-05-14T12:53:10.782147Z",
     "shell.execute_reply": "2025-05-14T12:53:10.781782Z"
    }
   },
   "outputs": [],
   "source": [
    "def id2symbol(reference, query, multi_select = \"first\"):\n",
    "     query = pd.DataFrame({\"gene_id\": query})\n",
    "\n",
    "     query = query.join(reference.set_index(\"gene_id\"), how = \"left\", on = \"gene_id\")\n",
    "     query = query[query.gene_id.duplicated(keep=multi_select) == False]\n",
    "     query = query[query.gene_name.duplicated(keep=multi_select) == False]\n",
    "     return query.gene_name.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114d16d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:10.783811Z",
     "iopub.status.busy": "2025-05-14T12:53:10.783545Z",
     "iopub.status.idle": "2025-05-14T12:53:10.833906Z",
     "shell.execute_reply": "2025-05-14T12:53:10.833161Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the id2symbol reference dataframe\n",
    "reference = pd.read_csv(\"human_id2symbol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd8f879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:10.836036Z",
     "iopub.status.busy": "2025-05-14T12:53:10.835721Z",
     "iopub.status.idle": "2025-05-14T12:53:15.829151Z",
     "shell.execute_reply": "2025-05-14T12:53:15.827350Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the SuperSCC markers\n",
    "superscc_markers = pd.read_pickle(\"SuperSCC_default_retrieving_method_top_20_markers.pkl\")\n",
    "for i in superscc_markers:\n",
    "    for j in superscc_markers[i]:\n",
    "        feature = superscc_markers[i][j].feature.tolist()\n",
    "        if(any([True if re.search(\"ENSG\\\\d+\", i) else False for i in feature])):\n",
    "            symbols = id2symbol(reference = reference, query = feature)\n",
    "        else:\n",
    "            symbols = feature\n",
    "        superscc_markers[i][j] = symbols\n",
    "\n",
    "# output the marker for running openAI API on local\n",
    "# with open(f\"{local}/superscc_markers.pkl\", \"wb\") as file:\n",
    "#     dill.dump(superscc_markers, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318e16f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:15.839866Z",
     "iopub.status.busy": "2025-05-14T12:53:15.839215Z",
     "iopub.status.idle": "2025-05-14T12:53:22.223513Z",
     "shell.execute_reply": "2025-05-14T12:53:22.222617Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the Scanpy-t test markers\n",
    "file = pd.read_csv(\"/mnt/disk5/zhongmin/superscc/结果位置/结果位置_3.csv\", encoding = \"GBK\", index_col = 0)\n",
    "scanpy_t_test_markers = dict()\n",
    "\n",
    "for idx, i in enumerate(file.t_test_path.values):\n",
    "    csv = pd.read_csv(i)\n",
    "    csv = csv.loc[(csv.logfoldchanges > 1) & (csv.pvals_adj < 0.05), :].sort_values(\"pvals_adj\", ascending = True)\n",
    "    key = file.index.tolist()[idx]\n",
    "    group_csv = csv.groupby(\"group\")\n",
    "    groups = group_csv.groups.keys()\n",
    "    for idx, group in enumerate(groups):\n",
    "        markers = group_csv.get_group(group).head(20).names.tolist()\n",
    "        if(any([True if re.search(\"ENSG\\\\d+\", i) else False for i in markers])):\n",
    "            markers = id2symbol(reference, markers)\n",
    "        if idx == 0:\n",
    "            scanpy_t_test_markers[key] = {group: markers}\n",
    "        else:\n",
    "            scanpy_t_test_markers[key].update({group: markers})\n",
    "\n",
    "# output the marker for running openAI API on local\n",
    "# with open(f\"{local}/scanpy_t_test_markers.pkl\", \"wb\") as file:\n",
    "#     dill.dump(scanpy_t_test_markers, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "722c8149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:43.045507Z",
     "iopub.status.busy": "2025-05-14T12:53:43.045248Z",
     "iopub.status.idle": "2025-05-14T12:53:52.772589Z",
     "shell.execute_reply": "2025-05-14T12:53:52.771645Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the Seurat-wilcox test markers\n",
    "files = list_files(path=\"seurat_res\", pattern=\".+csv$\")\n",
    "seurat_wilcox_markers = dict()\n",
    "\n",
    "for file in files:\n",
    "    csv = pd.read_csv(file)\n",
    "    csv = csv.loc[(csv.p_val_adj < 0.05) & (csv.avg_log2FC > 1)].sort_values(\"p_val_adj\", ascending = True)\n",
    "    name = re.sub(\"_seurat_feature.csv\", \"\", basename(file))\n",
    "\n",
    "    group_csv = csv.groupby(\"cluster\")\n",
    "\n",
    "    for idx, i in enumerate(group_csv.groups.keys()):\n",
    "        markers = group_csv.get_group(i).head(20).gene.values.tolist()\n",
    "        if(any([True if re.search(\"ENSG\\\\d+\", i) else False for i in markers])):\n",
    "            markers = id2symbol(reference, markers)\n",
    "        if idx == 0:\n",
    "            seurat_wilcox_markers[name] = {i: markers}\n",
    "        else:\n",
    "            seurat_wilcox_markers[name].update({i: markers})\n",
    "            \n",
    "# output the marker for running openAI API on local\n",
    "# with open(f\"{local}/seurat_wilcox_markers.pkl\", \"wb\") as file:\n",
    "#     dill.dump(seurat_wilcox_markers, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8c2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:53:58.975794Z",
     "iopub.status.busy": "2025-05-14T12:53:58.975653Z",
     "iopub.status.idle": "2025-05-14T12:53:58.979101Z",
     "shell.execute_reply": "2025-05-14T12:53:58.978748Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the shared cell type per method per dataset\n",
    "shared_cell_type = dict()\n",
    "for i in superscc_markers:\n",
    "    superscc_cell_type = set(superscc_markers[i].keys())\n",
    "    scanpy_t_test_cell_type = set(scanpy_t_test_markers[i].keys())\n",
    "    seurat_wilcox_cell_type = set(seurat_wilcox_markers[i].keys())\n",
    "    res = reduce(lambda x, y: x.intersection(y), [superscc_cell_type, scanpy_t_test_cell_type, seurat_wilcox_cell_type])\n",
    "    shared_cell_type[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e4d56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T15:59:45.524034Z",
     "iopub.status.busy": "2025-05-14T15:59:45.522573Z",
     "iopub.status.idle": "2025-05-14T15:59:45.536283Z",
     "shell.execute_reply": "2025-05-14T15:59:45.534085Z"
    }
   },
   "outputs": [],
   "source": [
    "# top 20 markers comparison\n",
    "model = \"deepseek-v3\" # qwen-max or GPT 4.1-mini\n",
    "api_key = \"#####\" # your api key\n",
    "base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "\n",
    "\n",
    "home = \"/home/fengtang/jupyter_notebooks/working_script/evulate_feature_selection/2nd_submssion/evaluation_res/original_prompt/DeepSeek-V3/10_markers\"\n",
    "os.chdir(home)\n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "\n",
    "for tempeture in [0.1, 0.5, 0.9]:\n",
    "\n",
    "    if not exists(f\"tempeature_{tempeture}\"):\n",
    "        os.makedirs(f\"tempeature_{tempeture}\")\n",
    "        os.chdir(f\"tempeature_{tempeture}\")\n",
    "    else:\n",
    "        os.chdir(f\"tempeature_{tempeture}\")\n",
    "    \n",
    "    for dataset in shared_cell_type:\n",
    "        \n",
    "        if not exists(f\"{dataset}\"):\n",
    "            os.makedirs(f\"{dataset}\")\n",
    "            os.chdir(f\"{dataset}\")\n",
    "        else:\n",
    "            os.chdir(f\"{dataset}\")\n",
    "\n",
    "        for cell_type in shared_cell_type[dataset]:\n",
    "            \n",
    "            print(f\"Processing with cell type '{cell_type}' in dataset '{dataset}'\")\n",
    "\n",
    "\n",
    "            try:\n",
    "                res = llm_compare_gene_set(model = model, \n",
    "                                        api_key = api_key, \n",
    "                                        base_url = base_url,\n",
    "                                        SuperSCC_gene_set = superscc_markers[dataset][cell_type],\n",
    "                                        Scanpy_t_test_gene_set = scanpy_t_test_markers[dataset][cell_type],\n",
    "                                        Seurat_wilcox_gene_set = seurat_wilcox_markers[dataset][cell_type],\n",
    "                                        cell_type = cell_type, \n",
    "                                        temperature = tempeture,\n",
    "                                        structure_output = True)\n",
    "            \n",
    "                cell_type = re.sub(\"/\", \"or\", cell_type)\n",
    "                with open(f\"{cell_type}_{dataset}_llm_evaulation.pkl\", \"wb\") as file:\n",
    "                    dill.dump(res, file)\n",
    "                time.sleep(10)\n",
    "            except:\n",
    "                cell_type = re.sub(\"/\", \"or\", cell_type)\n",
    "                with open(f\"{cell_type}_{dataset}_log.txt\", \"a\") as file:\n",
    "                    file.write(f\"Failure on cell type '{cell_type}' in dataset '{dataset}' \\n\")\n",
    "        \n",
    "        os.chdir(\"..\")\n",
    "        \n",
    "    os.chdir(home)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 markers comparison\n",
    "model = \"deepseek-v3\" # qwen-max or GPT 4.1-mini\n",
    "api_key = \"#####\"\n",
    "base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "\n",
    "\n",
    "for tempeture in [0.5]:\n",
    "\n",
    "    if not exists(f\"tempeature_{tempeture}\"):\n",
    "        os.makedirs(f\"tempeature_{tempeture}\")\n",
    "        os.chdir(f\"tempeature_{tempeture}\")\n",
    "    else:\n",
    "        os.chdir(f\"tempeature_{tempeture}\")\n",
    "    \n",
    "    for dataset in shared_cell_type:\n",
    "        \n",
    "        if not exists(f\"{dataset}\"):\n",
    "            os.makedirs(f\"{dataset}\")\n",
    "            os.chdir(f\"{dataset}\")\n",
    "        else:\n",
    "            os.chdir(f\"{dataset}\")\n",
    "\n",
    "        for cell_type in shared_cell_type[dataset]:\n",
    "            \n",
    "            print(f\"Processing with cell type '{cell_type}' in dataset '{dataset}'\")\n",
    "\n",
    "            try:\n",
    "                res = llm_compare_gene_set(model = model, \n",
    "                                        api_key = api_key, \n",
    "                                        base_url = base_url,\n",
    "                                        SuperSCC_gene_set = superscc_markers[dataset][cell_type][0:10],\n",
    "                                        Scanpy_t_test_gene_set = scanpy_t_test_markers[dataset][cell_type][0:10],\n",
    "                                        Seurat_wilcox_gene_set = seurat_wilcox_markers[dataset][cell_type][0:10],\n",
    "                                        cell_type = cell_type, \n",
    "                                        temperature = tempeture,\n",
    "                                        structure_output = True)\n",
    "            \n",
    "                cell_type = re.sub(\"/\", \"or\", cell_type)\n",
    "                with open(f\"{cell_type}_{dataset}_llm_evaulation.pkl\", \"wb\") as file:\n",
    "                    dill.dump(res, file)\n",
    "                # time.sleep(10)\n",
    "            except:\n",
    "                cell_type = re.sub(\"/\", \"or\", cell_type)\n",
    "                with open(f\"{cell_type}_{dataset}_log.txt\", \"a\") as file:\n",
    "                    file.write(f\"Failure on cell type '{cell_type}' in dataset '{dataset}' \\n\")\n",
    "        \n",
    "        os.chdir(\"..\")\n",
    "        \n",
    "    os.chdir(home)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SuperSCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
